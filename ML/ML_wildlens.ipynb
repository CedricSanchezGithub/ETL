{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fee02c-ad40-40d1-973b-ac3ad9cc5d90",
   "metadata": {},
   "source": [
    "1# Classification multiclasses des images du dataset 5 flowers\n",
    "\n",
    "# Data Augmentation + Transfer Learning avec MobileNetV3Small + Fine Tuning\n",
    "\n",
    "- Auteur : Laurent PISSOT\n",
    "- Date : 12 Mai 2025\n",
    "- Statut : **Validé**\n",
    "- Références : <br>https://docs.pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_small.html, <br> https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/..., <br> https://docs.pytorch.org/vision/0.22/models.html, <br> https://pypi.org/project/torch/2.6.0/, <br> requêtes Copilot !"
   ]
  },
  {
   "cell_type": "code",
   "id": "863496cc-659d-4ed5-8472-6a9c98008afc",
   "metadata": {},
   "source": [
    "!python --version"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60f9fbf7-03d1-4b71-a31a-a63de611b477",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4402f02-de6f-4641-b5c6-1abb67ffe94a",
   "metadata": {},
   "source": [
    "MobileNetV3, une architecture de pointe pour des modèles de deep learning efficaces conçus pour les appareils mobiles. Il s’agit de la troisième génération de la famille MobileNet.\n",
    "\n",
    "Les MobileNet sont des réseaux neuronaux convolutifs (CNN) légers optimisés pour la vitesse et la précision. MobileNetV3 introduit de nouvelles améliorations de l’architecture, telles que la recherche d’architecture neuronale (NAS) sensible à la plate-forme et NetAdapt, afin d’améliorer encore les performances.\n",
    "\n",
    "**Qu'est-ce que MobileNet ?**<br>\n",
    "MobileNet est une famille de réseaux neuronaux conçus pour une inférence efficace sur les appareils mobiles et intégrés. Le MobileNetV1 original a introduit une technique appelée convolutions séparables en profondeur, qui a considérablement réduit le nombre de calculs par rapport aux convolutions traditionnelles.\n",
    "\n",
    "Les MobileNet sont particulièrement bien adaptés aux tâches telles que la classification d’images, la détection d’objets et la segmentation sémantique sur des appareils disposant d’une puissance de calcul limitée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8b102-e7d9-41a8-9a18-5c5521bbe579",
   "metadata": {},
   "source": [
    "**MobileNetV1 vs V2 vs V3 : quelle est la différence ?**\n",
    "\n",
    "**MobileNetV1** : Introduction de convolutions séparables en profondeur pour réduire le calcul et la taille du modèle.\n",
    "\n",
    "**MobileNetV2** : Ajout de résidus inversés et de goulets d’étranglement linéaires pour rendre le réseau plus efficace.\n",
    "\n",
    "**MobileNetV3** : Combine le meilleur des deux versions précédentes et les améliore avec :\n",
    "\n",
    "- NAS sensible à la plate-forme pour optimiser l’architecture des processeurs mobiles.\n",
    "- NetAdapt pour affiner les couches réseau pour plus d’efficacité.\n",
    "- Modules Squeeze-and-Excite (SE) pour stimuler l’apprentissage des fonctionnalités.\n",
    "- Fonction d’activation H-Swish pour améliorer l’efficacité du modèle."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timer(line, cell):\n",
    "    start = time.time()\n",
    "    exec(cell, globals())\n",
    "    end = time.time()\n",
    "    print(f\"⏱ Temps d'exécution de la cellule : {end - start:.2f} secondes\")\n",
    "\n"
   ],
   "id": "363bdc0c9985d0ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14cacd11-e26f-49ac-b686-c7942f5b3c36",
   "metadata": {},
   "source": [
    "## Étape 1 : Configuration de l'environnement d'exécution"
   ]
  },
  {
   "cell_type": "code",
   "id": "234ca7c5-7f20-458c-8db5-999780c8178f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pathlib\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "start_time_notebook = time.time()\n",
    "# Définition des constantes\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 10\n",
    "NB_CLASSES = 17"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c23b0705-e922-4c77-b245-d2798c92d0ff",
   "metadata": {},
   "source": [
    "torch.__version__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ce561af-4085-4890-aa4b-40d94a0e67be",
   "metadata": {},
   "source": [
    "#### Définition des fonctions locales"
   ]
  },
  {
   "cell_type": "code",
   "id": "09a57d7d-933f-498d-b6f9-01327430e9cf",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
    "def f_make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
    "    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "      If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "      will be used.\n",
    "\n",
    "      Args:\n",
    "        y_true: Array of truth labels (must be same shape as y_pred).\n",
    "        y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "        classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "        figsize: Size of output figure (default=(10, 10)).\n",
    "        text_size: Size of output figure text (default=15).\n",
    "        norm: normalize values or not (default=False).\n",
    "        savefig: save confusion matrix to file (default=False).\n",
    "  \n",
    "      Returns:\n",
    "        A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "\n",
    "      Example usage:\n",
    "        make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
    "                              y_pred=y_preds, # predicted labels\n",
    "                              classes=class_names, # array of class label names\n",
    "                              figsize=(15, 15),\n",
    "                              text_size=10)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create the confustion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "    n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "    # Plot the figure and make it pretty\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "  \n",
    "    # Are there a list of classes?\n",
    "    if classes:  labels = classes\n",
    "    else:  labels = np.arange(cm.shape[0])\n",
    "  \n",
    "    # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           ylabel=\"True label\",\n",
    "           xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "           yticks=np.arange(n_classes), \n",
    "           xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "           yticklabels=labels)\n",
    "  \n",
    "    # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
    "    plt.xticks(rotation=70, fontsize=text_size)\n",
    "    plt.yticks(fontsize=text_size)\n",
    "\n",
    "    # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if norm:\n",
    "            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                     size=text_size)\n",
    "        else:\n",
    "           plt.text(j, i, f\"{cm[i, j]}\",\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                    size=text_size)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2c5a4c4-4cda-4bda-b04d-928be8ef1ee5",
   "metadata": {},
   "source": [
    "%%timer\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Transformations standards pour MobileNetV3\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # redimensionnement obligatoire\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Pas d’augmentation, donc mêmes transformations pour val/test\n",
    "transform_val = transform_train\n",
    "transform_test = transform_train"
   ],
   "id": "6bf7b858ef8cbedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chargement des données",
   "id": "fbf830bd00278afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "engine = create_engine(\"mysql+pymysql://root:root@localhost:3306/wildlens\")\n",
    "df_all = pd.read_sql(\"SELECT * FROM wildlens_images\", engine)\n",
    "df_labels = pd.read_sql(\"SELECT id_espece, nom_fr FROM wildlens_facts\", engine)\n",
    "df_all = pd.merge(df_all, df_labels, on=\"id_espece\", how=\"left\")\n",
    "\n",
    "# Étape 1 : récupérer les ID d'espèce uniques (par sécurité)\n",
    "unique_species_ids = sorted(df_all['id_espece'].unique())\n",
    "\n",
    "# Étape 2 : dictionnaire de mappage\n",
    "id_to_class = {id_: idx for idx, id_ in enumerate(unique_species_ids)}\n",
    "\n",
    "# Étape 3 : conversion dans le DataFrame\n",
    "df_all['label_class'] = df_all['id_espece'].map(id_to_class)\n",
    "\n",
    "# Split + copie propre\n",
    "train_df = df_all[df_all[\"id_etat\"] == 1].copy()\n",
    "val_df = df_all[df_all[\"id_etat\"] == 2].copy()\n",
    "test_df = df_all[df_all[\"id_etat\"] == 3].copy()\n",
    "\n"
   ],
   "id": "88bcfd6558e53751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WildLensDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.base_path = Path(base_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Forcer index entier (résout 90% des cas)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "        elif isinstance(idx, (list, tuple)):\n",
    "            idx = idx[0]\n",
    "\n",
    "        # ⚠️ Debug temporaire : afficher l'index et taille max\n",
    "        if idx >= len(self.df):\n",
    "            raise IndexError(f\"Index {idx} hors limites (longueur dataset : {len(self.df)})\")\n",
    "\n",
    "        try:\n",
    "            row = self.df.iloc[int(idx)]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur à l'accès iloc[{idx}]\")\n",
    "            raise e\n",
    "\n",
    "        image_path_bdd = self.base_path / row[\"image\"]\n",
    "        label = row[\"label_class\"]\n",
    "\n",
    "        try:\n",
    "            image_bdd = Image.open(image_path_bdd).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur d'ouverture d'image : {image_path_bdd}\")\n",
    "            raise e\n",
    "\n",
    "        if self.transform:\n",
    "            image_bdd = self.transform(image_bdd)\n",
    "        return image_bdd, label\n",
    "\n",
    "\n",
    "# train_dataset est un objet PyTorch, une interface permettant de charger dynamiquement les images et labels pour l’entraînement\n",
    "\n",
    "# Créer les datasets avec votre classe WildLensDataset\n",
    "train_dataset = WildLensDataset(train_df, \"../ETL/ressource/image/augmented_train\", transform_train)\n",
    "val_dataset = WildLensDataset(val_df, \"../ETL/ressource/image/augmented_train\", transform_val)\n",
    "test_dataset = WildLensDataset(test_df, \"../ETL/ressource/image/augmented_train\", transform_test)\n",
    "\n",
    "# Créer les DataLoaders avec les bons datasets\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ],
   "id": "d8b340fff64f44ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔍 Test d’accès direct sans DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "img, lbl = train_dataset[0]\n",
    "print(\"Image OK :\", img.shape)\n",
    "print(\"Label OK :\", lbl)\n",
    "\n",
    "img, lbl = val_dataset[0]\n",
    "print(\"✔️ Test val_dataset[0] ok :\", img.shape, lbl)\n",
    "\n"
   ],
   "id": "e851e992e581d296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_df.head())\n",
    "print(train_df.columns)\n"
   ],
   "id": "525834ba6d56a881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Répartition des classes dans le train\n",
    "class_counts = train_df[\"nom_fr\"].value_counts().sort_index()\n",
    "class_counts.plot(kind=\"bar\", figsize=(10, 4))\n",
    "plt.title(\"Répartition des images par classe (nom_fr) - TRAIN\")\n",
    "plt.xlabel(\"Animaux\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "df_all[\"id_etat\"].replace({1: \"train\", 2: \"val\", 3: \"test\"}).value_counts().plot(\n",
    "    kind=\"bar\", title=\"Répartition globale des images par split\"\n",
    ")\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df_all[\"split\"] = df_all[\"id_etat\"].replace({1: \"train\", 2: \"val\", 3: \"test\"})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df_all, x=\"nom_fr\", hue=\"split\")\n",
    "plt.title(\"Répartition des images par classe et split\")\n",
    "plt.xlabel(\"ID de l'espèce\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.legend(title=\"Split\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f8f07a12920e9703",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f286c53-1fe6-44fd-ab33-dd42676d3638",
   "metadata": {},
   "source": [
    "## Étape 2 : Chargement du modèle pré entrainé MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "id": "37015d8c-3eec-444a-924c-3d93a4cff5a1",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Load MobileNetV3-Large pretrained on ImageNet\n",
    "# Pre-trained Model: The pretrained=True argument loads a model trained on ImageNet.\n",
    "#mobilenet_v3_large = models.mobilenet_v3_large(pretrained=True)  # Use mobilenet_v3_small for the smaller version\n",
    "mobilenet_v3 = models.mobilenet_v3_small(pretrained=True, weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "# Modify the final layer for a custom number of classes (e.g., 10)\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, NB_CLASSES)   # mobilenet_v3_small\n",
    "\n",
    "# Verify the modified model\n",
    "print(mobilenet_v3)\n",
    "\n",
    "#train on GPU if CUDA is available, else on CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "#print training information\n",
    "print(\"\")\n",
    "if torch.cuda.is_available():  hardware = \"GPU \" + str(device) \n",
    "else:    hardware = \"CPU (CUDA was not found)\" \n",
    "print(\"Training information:\")\n",
    "print(\"iterations:\", NB_EPOCHS)\n",
    "print(\"batch size:\", BATCH_SIZE)\n",
    "print(\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2511ba35-13b9-480e-bec8-75f23f694dc6",
   "metadata": {},
   "source": [
    "### Étape 3 : Réglage de précision du modèle MobileNetV3 sur les couches de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406f58c-f359-4d8b-be7b-fe8114ae40fa",
   "metadata": {},
   "source": [
    "* Prétraitement et enrichissement des données (Data Augmentation) :"
   ]
  },
  {
   "cell_type": "code",
   "id": "87d7da81-4d81-42a3-919b-a597c941ef59",
   "metadata": {},
   "source": [
    "# DIR_IMG_TRAIN = '../ETL/ressource/image/augmented_train'  # localisation a adapter\n",
    "# DIR_IMG_TEST = 'flower_photos_test'  # localisation a adapter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "debffc3b-0b75-4316-a55b-25e5ff731cd5",
   "metadata": {},
   "source": [
    "# %%timer\n",
    "# # Pour l'entraînement (avec augmentation)\n",
    "# # 1. Image Preprocessing: The input image must be resized, normalized, and converted to a tensor.\n",
    "# NORM_MEANS = (0.485, 0.456, 0.406) #precomputed channel means of ImageNet(train) for normalization\n",
    "# NORM_STDS = (0.229, 0.224, 0.225) #precomputed standard deviations\n",
    "#\n",
    "# train_transform = transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),  # Resize image to 224x224\n",
    "#                                       #transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.RandomRotation(degrees=15),\n",
    "#                                       transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize(mean=NORM_MEANS, std=NORM_STDS)])\n",
    "#\n",
    "# # Load dataset et configure DataLoaders\n",
    "# image_train_path = DIR_IMG_TRAIN\n",
    "# train_dataset = datasets.ImageFolder(root=image_train_path, transform=train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f48e4a6-1133-4bfb-b325-123c86e0993d",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Constantes pour l'apprentissage\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 10\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32  # ou la valeur que tu veux\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc9ad512-b928-4ae3-b653-4bfb50c05dd4",
   "metadata": {},
   "source": [
    "%%timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Constantes\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modèle\n",
    "mobilenet_v3 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, 17)\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "# Optimiseur & fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v3.parameters(), lr=LR)\n",
    "\n",
    "# Suivi\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0  # suivi du meilleur score de validation\n",
    "\n",
    "# Entraînement\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    mobilenet_v3.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Évaluation validation\n",
    "    mobilenet_v3.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mobilenet_v3(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Sauvegarde du meilleur modèle\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(mobilenet_v3.state_dict(), \"best_model_wildlens.pt\")\n",
    "        print(f\"💾 Nouveau meilleur modèle sauvegardé à l’époque {epoch+1} (val_acc = {val_accuracy:.2f}%)\")\n",
    "\n",
    "    # Log\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NB_EPOCHS} - Loss: {epoch_loss:.4f} - Train Acc: {epoch_accuracy:.2f}% - Val Acc: {val_accuracy:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 📊 Courbes\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.title(\"Train Loss par époque\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Acc\", marker='o')\n",
    "plt.plot(val_accuracies, label=\"Val Acc\", marker='s')\n",
    "plt.title(\"Accuracy (Train vs Val)\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b7ca6bb9f5bfbc81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mapping label_class (entier 0 → 16) vers nom_fr\n",
    "idx_to_label = {\n",
    "    id_to_class[id_espece]: nom_fr\n",
    "    for id_espece, nom_fr in zip(df_labels[\"id_espece\"], df_labels[\"nom_fr\"])\n",
    "}\n"
   ],
   "id": "498d3dc576b961aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Chargement du modèle entraîné\n",
    "mobilenet_v3.load_state_dict(torch.load(\"best_model_wildlens.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# Évaluation sur test set\n",
    "correct_test, total_test = 0, 0\n",
    "true_test, pred_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        true_test.extend(labels.cpu().numpy())\n",
    "        pred_test.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f\"✅ Accuracy finale sur le jeu de test : {test_accuracy:.2f}%\")\n"
   ],
   "id": "2c37dea3f9170362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(true_test, pred_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[idx_to_label[i] for i in range(len(cm))])\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(xticks_rotation='vertical', ax=ax, cmap=\"Greens\")\n",
    "plt.title(\"🧪 Matrice de confusion - Test final\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "6f064fe91f1fd00f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbba24f4-6d45-4cce-84ab-3f1b66c9f8da",
   "metadata": {},
   "source": [
    "### Étape 4 : Évaluation du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "id": "feb21bcd-68a0-4270-9d78-cc590971ddfe",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Evaluation Mode: Always set the model to evaluation mode using model.eval() to disable dropout and batch normalization updates.\n",
    "mobilenet_v3.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Train Accuracy: {accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4f4c-7330-4330-8fcc-978740ab11b3",
   "metadata": {},
   "source": [
    "* Mesure des performances en TEST en itérant sur les données du dataset `test_loader` :"
   ]
  },
  {
   "cell_type": "code",
   "id": "3856e7c6-b8fe-43db-910f-60d5620ac9f0",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Predict (Make Predictions on New Data)\n",
    "#mobilenet_v3.eval()  # Set to evaluation mode\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():  # Disable gradient tracking for efficiency\n",
    "    for inputs, labels in test_loader:  # Assuming `test_loader` is your DataLoader\n",
    "        predictions = mobilenet_v3(inputs)  # Forward pass\n",
    "        predicted = torch.argmax(predictions, dim=1)  # Get class labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ceea33e3-e62c-4a60-adc0-c2f6b3f0002e",
   "metadata": {},
   "source": [
    "### Etape 5 : Inférences unitaires et globales à l'ensemble de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9721f7-c1fc-4a2e-9e12-1a475d22e3ca",
   "metadata": {},
   "source": [
    "* Nouvelle évaluation globale sur les données de TEST en itérant manuellement sur les images physiques :"
   ]
  },
  {
   "cell_type": "code",
   "id": "74812557-01a5-43c3-9bb2-cb45ec0b5df3",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ✅ Charge le modèle entraîné\n",
    "mobilenet_v3.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# 🖼️ Chemin de l’image à tester\n",
    "image_path = \"../ETL/ressource/image/augmented_train/muledeer/aug_0_2097.jpg\"  # ← À modifier\n",
    "\n",
    "# 🔄 Prétraitement\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 🔮 Prédiction\n",
    "with torch.no_grad():\n",
    "    output = mobilenet_v3(input_tensor)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    confidence, predicted_class = torch.max(probs, dim=1)\n",
    "\n",
    "# 🧾 Affichage\n",
    "print(f\"✅ Classe prédite : {idx_to_label[predicted_class.item()]}\")\n",
    "print(f\"🔢 Score de confiance : {confidence.item()*100:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25ae6ce6-f93b-4c42-941e-1eee7de2e2a5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
