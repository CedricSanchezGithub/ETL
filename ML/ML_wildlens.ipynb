{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification multiclasses des images d'animaux sauvages avec WildLens\n",
    "\n",
    "## R√©sum√©\n",
    "Ce notebook impl√©mente un mod√®le de classification d'images pour identifier 17 esp√®ces d'animaux sauvages. Il utilise une approche de transfer learning avec MobileNetV3Small, suivie d'un fine-tuning pour optimiser les performances.\n",
    "\n",
    "## Informations g√©n√©rales\n",
    "- **Auteur** : C√©dric Sanchez, gr√¢ce aux travaux de Laurent PISSOT\n",
    "- **Date** : 12 Mai 2025\n",
    "- **Statut** : **Valid√©**\n",
    "- **Objectif** : Cr√©er un mod√®le de classification d'images l√©ger et performant pour l'application mobile WildLens\n",
    "- **R√©f√©rences** : \n",
    "  - [Documentation MobileNetV3Small](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_small.html)\n",
    "  - [Impl√©mentation PyTorch de MobileNetV3](https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/)\n",
    "  - [Documentation des mod√®les PyTorch](https://docs.pytorch.org/vision/0.22/models.html)\n",
    "  - [PyTorch 2.6.0](https://pypi.org/project/torch/2.6.0/)"
   ],
   "id": "305d127306ac20c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Introduction",
   "id": "3bf95dbae52298d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MobileNetV3, une architecture de pointe pour des mod√®les de deep learning efficaces con√ßus pour les appareils mobiles. Il s‚Äôagit de la troisi√®me g√©n√©ration de la famille MobileNet.\n",
    "\n",
    "Les MobileNet sont des r√©seaux neuronaux convolutifs (CNN) l√©gers optimis√©s pour la vitesse et la pr√©cision. MobileNetV3 introduit de nouvelles am√©liorations de l‚Äôarchitecture, telles que la recherche d‚Äôarchitecture neuronale (NAS) sensible √† la plate-forme et NetAdapt, afin d‚Äôam√©liorer encore les performances.\n",
    "\n",
    "**Qu'est-ce que MobileNet ?**<br>\n",
    "MobileNet est une famille de r√©seaux neuronaux con√ßus pour une inf√©rence efficace sur les appareils mobiles et int√©gr√©s. Le MobileNetV1 original a introduit une technique appel√©e convolutions s√©parables en profondeur, qui a consid√©rablement r√©duit le nombre de calculs par rapport aux convolutions traditionnelles.\n",
    "\n",
    "Les MobileNet sont particuli√®rement bien adapt√©s aux t√¢ches telles que la classification d‚Äôimages, la d√©tection d‚Äôobjets et la segmentation s√©mantique sur des appareils disposant d‚Äôune puissance de calcul limit√©e."
   ],
   "id": "63fc479bd1dc7a87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**MobileNetV1 vs V2 vs V3 : quelle est la diff√©rence ?**\n",
    "\n",
    "**MobileNetV1** : Introduction de convolutions s√©parables en profondeur pour r√©duire le calcul et la taille du mod√®le.\n",
    "\n",
    "**MobileNetV2** : Ajout de r√©sidus invers√©s et de goulets d‚Äô√©tranglement lin√©aires pour rendre le r√©seau plus efficace.\n",
    "\n",
    "**MobileNetV3** : Combine le meilleur des deux versions pr√©c√©dentes et les am√©liore avec :\n",
    "\n",
    "- NAS sensible √† la plate-forme pour optimiser l‚Äôarchitecture des processeurs mobiles.\n",
    "- NetAdapt pour affiner les couches r√©seau pour plus d‚Äôefficacit√©.\n",
    "- Modules Squeeze-and-Excite (SE) pour stimuler l‚Äôapprentissage des fonctionnalit√©s.\n",
    "- Fonction d‚Äôactivation H-Swish pour am√©liorer l‚Äôefficacit√© du mod√®le."
   ],
   "id": "f4c09fa4e7341af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## √âtape 1 : Configuration de l'environnement d'ex√©cution\n",
    "\n",
    "Cette section initialise l'environnement de travail pour l'entra√Ænement du mod√®le :\n",
    "- Importation des biblioth√®ques n√©cessaires (PyTorch, torchvision, sklearn, etc.)\n",
    "- D√©finition des constantes globales pour l'entra√Ænement\n",
    "- Configuration d'un timer pour mesurer les performances d'ex√©cution\n",
    "\n",
    "### Param√®tres principaux\n",
    "- **IMG_SIZE** : Taille des images d'entr√©e (224x224 pixels)\n",
    "- **BATCH_SIZE** : Nombre d'images trait√©es par lot (32)\n",
    "- **LR** : Taux d'apprentissage initial (0.001)\n",
    "- **NB_EPOCHS** : Nombre d'√©poques d'entra√Ænement (10)\n",
    "- **NB_CLASSES** : Nombre de classes √† pr√©dire (17 esp√®ces d'animaux)"
   ],
   "id": "319d30b68fd81260"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "debut_notebook = time.time()\n",
    "from ML.utils.utils import afficher_matrice_confusion\n",
    "from ML.utils.utils import generer_rapport_classification\n",
    "from ML.utils.utils import afficher_courbes_entrainement\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import ImageStat\n",
    "from PIL import Image\n",
    "from sqlalchemy import create_engine\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@register_cell_magic\n",
    "def timer(line, cell):\n",
    "    start = time.time()\n",
    "    exec(cell, globals())\n",
    "    end = time.time()\n",
    "    print(f\"‚è± Temps d'ex√©cution de la cellule : {end - start:.2f} secondes\")\n",
    "\n",
    "# D√©finition des hyperparam√®tres pour l'entra√Ænement\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 20\n",
    "NB_FINE_EPOCHS = 20\n",
    "NB_CLASSES = 17"
   ],
   "id": "494a6639841775f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# D√©tection d'images de qualit√© douteuse\n",
    "Cette cellule analyse le jeu de donn√©es pour d√©tecter les images qui pourraient √™tre probl√©matiques pour l'entra√Ænement du mod√®le.\n",
    "\n",
    "\n",
    "## Fonctionnement\n",
    "1. Parcours du dataset : Exploration de chaque classe d'image (sous-dossiers)\n",
    "2. Analyse statistique : Conversion en niveaux de gris et calcul de deux indicateurs cl√©s:\n",
    "    - `luminosit√©` : luminosit√© moyenne de l'image\n",
    "    - `stddev` : √©cart-type (mesure du contraste)\n",
    "3. D√©tection : Une image est marqu√©e comme \"suspecte\" si:\n",
    "luminosit√© < 40 (trop sombre)\n",
    "stddev < 10 (contraste insuffisant)\n",
    "## R√©sultats\n",
    "- D√©compte des images suspectes (total et par classe)\n",
    "- Cr√©ation d'un DataFrame avec toutes les statistiques pour analyse ult√©rieure\n",
    "Cette analyse permet d'identifier les images de mauvaise qualit√© qui pourraient nuire √† l'apprentissage du mod√®le."
   ],
   "id": "e709cb251dba19e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# R√©pertoire des donn√©es\n",
    "data_dir = \"../ETL/ressource/image/augmented_train\"\n",
    "\n",
    "# Initialisation des r√©sultats\n",
    "results = []\n",
    "\n",
    "# Parcours des sous-dossiers (une classe = un dossier)\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # ignorer les fichiers\n",
    "\n",
    "    for img_file in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"L\")\n",
    "            stat = ImageStat.Stat(img)\n",
    "            brightness = stat.mean[0]\n",
    "            stddev = stat.stddev[0]\n",
    "\n",
    "            results.append({\n",
    "                \"chemin\": img_path,\n",
    "                \"classe\": class_name,\n",
    "                \"luminosite\": brightness,\n",
    "                \"stddev\": stddev,\n",
    "                \"suspecte\": brightness < 40 or stddev < 10\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lecture image {img_path} : {e}\")\n",
    "\n",
    "# R√©sultats sous forme de DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nüìä Images suspectes d√©tect√©es : {df['suspecte'].sum()} sur {len(df)} images\")\n",
    "\n",
    "# Nombre d‚Äôimages suspectes par classe\n",
    "print(\"\\nNombre d‚Äôimages suspectes par classe :\")\n",
    "print(df[df['suspecte']].groupby('classe').size())\n",
    "\n",
    "# Affichage d‚Äôun √©chant\n"
   ],
   "id": "44830ea4c0a691b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# V√©rifie qu'on a d√©j√† un DataFrame `df` avec la colonne 'suspecte'\n",
    "df_suspect = df[df['suspecte']].copy()\n",
    "n = min(20, len(df_suspect))\n",
    "\n",
    "if n == 0:\n",
    "    print(\"‚úÖ Aucune image suspecte √† afficher.\")\n",
    "else:\n",
    "    print(f\"üé® Affichage de {n} image(s) suspecte(s) :\")\n",
    "    sample = df_suspect.sample(n=n, random_state=1)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    for i, row in enumerate(sample.itertuples()):\n",
    "        img = Image.open(row.chemin).convert(\"RGB\")\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{row.classe}\\nLum: {row.luminosite:.0f} | Std: {row.stddev:.0f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "3cdbe7548aeddb2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# D√©tection d'empreintes dans la neige\n",
    "\n",
    "Cette cellule identifie les images contenant probablement de la neige gr√¢ce √† deux crit√®res colorim√©triques : la pr√©sence de pixels blancs purs (`seuil_blanc`) et la dominance de la composante bleue (`bleu_dominant`). Les images d√©tect√©es sont ensuite ajout√©es au DataFrame avec un marqueur `neige_v2` et visualis√©es pour v√©rification."
   ],
   "id": "4994696fec2556d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# D√©tection am√©lior√©e d'empreintes dans la neige avec filtre sur \"blanc pur\" et \"bleu dominant\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def est_neigeuse_avancee(img_path, seuil_blanc=230, ratio_blanc_min=0.2, ratio_bleu_dominant=0.6):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB').resize((128, 128))\n",
    "        arr = np.asarray(img).astype(np.float32)\n",
    "\n",
    "        # D√©tection de \"pixels blancs purs\"\n",
    "        masque_blanc = (arr[..., 0] > seuil_blanc) & (arr[..., 1] > seuil_blanc) & (arr[..., 2] > seuil_blanc)\n",
    "        ratio_blanc = masque_blanc.sum() / (128 * 128)\n",
    "\n",
    "        # D√©tection de \"pixels bleus dominants\"\n",
    "        bleu_dominant = (arr[..., 2] > arr[..., 0] + 15) & (arr[..., 2] > arr[..., 1] + 15)\n",
    "        ratio_bleu = bleu_dominant.sum() / (128 * 128)\n",
    "\n",
    "        return (ratio_blanc > ratio_blanc_min) or (ratio_bleu > ratio_bleu_dominant)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {img_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Application √† toutes les images\n",
    "df['neige_v2'] = df['chemin'].apply(est_neigeuse_avancee)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "nb_detectees = df['neige_v2'].sum()\n",
    "print(f\"‚ùÑÔ∏è Neige d√©tect√©e (version avanc√©e) sur {nb_detectees} images / {len(df)}\")\n",
    "\n",
    "# Affichage des images (max 20)\n",
    "echantillon = df[df['neige_v2']].sample(n=min(20, nb_detectees), random_state=42)\n",
    "\n",
    "plt.figure(figsize=(31, 8))\n",
    "for i, row in enumerate(echantillon.itertuples()):\n",
    "    img = Image.open(row.chemin)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(row.classe, fontsize=9)\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Empreintes d√©tect√©es comme dans la neige (d√©tection am√©lior√©e)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac7719444c01975e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Documentation**\n",
    "\n",
    "Cette transformation normalise les valeurs des pixels de l'image en utilisant la moyenne et l'√©cart-type par canal (R, G, B) calcul√©s sur ImageNet.\n",
    "\n",
    "### **Fonctionnement :**\n",
    "\n",
    "\n",
    "- Pour chaque pixel et canal:\n",
    "\n",
    "    ` pixel_normalis√© = (pixel_original - moyenne) / √©cart_type`\n",
    "\n",
    "* Canal R: `(R - 0.485) / 0.229`\n",
    "* Canal G: `(G - 0.456) / 0.224`\n",
    "* Canal B: `(B - 0.406) / 0.225`\n",
    "\n",
    "Pourquoi ces valeurs? Ces moyennes et √©carts-types correspondent aux statistiques d'ImageNet, le dataset sur lequel MobileNetV3 a √©t√© pr√©-entra√Æn√©.\n",
    "\n",
    "### Avantages:\n",
    "\n",
    "* Acc√©l√®re la convergence de l'entra√Ænement\n",
    "* Am√©liore la pr√©cision des pr√©dictions\n",
    "* N√©cessaire pour utiliser correctement les mod√®les pr√©-entra√Æn√©s"
   ],
   "id": "c4c5be02004c8c12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Transformations standards pour MobileNetV3\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # redimensionnement obligatoire\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Pas d‚Äôaugmentation, donc m√™mes transformations pour val/test\n",
    "transform_val = transform_train\n",
    "transform_test = transform_train"
   ],
   "id": "6bf7b858ef8cbedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pr√©paration des donn√©es pour le mod√®le de classification\n",
    "Ce code effectue le chargement et la pr√©paration des donn√©es d'images depuis la base de donn√©es MySQL pour le mod√®le de classification d'empreintes d'animaux.\n",
    "\n",
    "Le code r√©alise quatre op√©rations principales:\n",
    "\n",
    "\n",
    "1. Chargement des donn√©es depuis la base MySQL et fusion des tables d'images et d'√©tiquettes\n",
    "2. Conversion des ID d'esp√®ces en indices num√©riques s√©quentiels (0, 1, 2...) n√©cessaires pour l'entra√Ænement\n",
    "3. Cr√©ation de la colonne label_class qui servira d'√©tiquette pour le mod√®le d'apprentissage\n",
    "4. Division des donn√©es en trois ensembles distincts selon la valeur de id_etat (1=train, 2=validation, 3=test)"
   ],
   "id": "fbf830bd00278afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "engine = create_engine(\"mysql+pymysql://root:root@localhost:3306/wildlens\")\n",
    "df_all = pd.read_sql(\"SELECT * FROM wildlens_images\", engine)\n",
    "df_labels = pd.read_sql(\"SELECT id_espece, nom_fr FROM wildlens_facts\", engine)\n",
    "df_all = pd.merge(df_all, df_labels, on=\"id_espece\", how=\"left\")\n",
    "\n",
    "# √âtape 1 : r√©cup√©rer les ID d'esp√®ce uniques (par s√©curit√©)\n",
    "unique_species_ids = sorted(df_all['id_espece'].unique())\n",
    "\n",
    "# √âtape 2 : dictionnaire de mappage\n",
    "id_to_class = {id_: idx for idx, id_ in enumerate(unique_species_ids)}\n",
    "\n",
    "# √âtape 3 : conversion dans le DataFrame\n",
    "df_all['label_class'] = df_all['id_espece'].map(id_to_class)\n",
    "\n",
    "# Split + copie propre\n",
    "train_df = df_all[df_all[\"id_etat\"] == 1].copy()\n",
    "val_df = df_all[df_all[\"id_etat\"] == 2].copy()\n",
    "test_df = df_all[df_all[\"id_etat\"] == 3].copy()"
   ],
   "id": "88bcfd6558e53751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_df.to_json(\"train_df.json\", orient=\"records\", lines=True)\n",
    "# val_df.to_json(\"val_df.json\", orient=\"records\", lines=True)\n",
    "# test_df.to_json(\"test_df.json\", orient=\"records\", lines=True)\n",
    "\n",
    "# train_df = pd.read_json(\"train_df.json\", lines=True)\n",
    "# val_df = pd.read_json(\"val_df.json\", lines=True)\n",
    "# test_df = pd.read_json(\"test_df.json\", lines=True)\n"
   ],
   "id": "87d79f9174b85f84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  WildLensDataset\n",
    "\n",
    "Cette classe h√©rite de `torch.utils.data.Dataset` et permet de charger des images √† partir d‚Äôun DataFrame.\n",
    "\n",
    "###  Param√®tres\n",
    "- `dataframe` *(pd.DataFrame)* : table contenant les colonnes `image` (chemin relatif) et `label_class` (√©tiquette).\n",
    "- `base_path` *(str ou Path)* : dossier racine contenant les images.\n",
    "- `transform` *(callable, optionnel)* : transformations √† appliquer aux images (ex : redimensionnement, normalisation).\n",
    "\n",
    "###  M√©thodes\n",
    "- `__len__()` : retourne le nombre d‚Äô√©l√©ments dans le dataset.\n",
    "- `__getitem__(idx)` :\n",
    "  - Charge l‚Äôimage et le label correspondant √† l‚Äôindice `idx`.\n",
    "  - V√©rifie que l‚Äôindice est valide.\n",
    "  - Applique les transformations si sp√©cifi√©es.\n",
    "\n",
    "###  Gestion des erreurs\n",
    "- Conversion s√©curis√©e de l‚Äôindex (`int`, `Tensor`, `tuple`, etc.)\n",
    "- Messages explicites en cas de probl√®me d‚Äôacc√®s au DataFrame ou d‚Äôouverture de fichier image.\n"
   ],
   "id": "74bf4af304e3c53f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WildLensDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.base_path = Path(base_path)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Forcer index entier (r√©sout 90% des cas)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "        elif isinstance(idx, (list, tuple)):\n",
    "            idx = idx[0]\n",
    "\n",
    "        #  Debug temporaire : afficher l'index et taille max\n",
    "        if idx >= len(self.df):\n",
    "            raise IndexError(f\"Index {idx} hors limites (longueur dataset : {len(self.df)})\")\n",
    "\n",
    "        try:\n",
    "            row = self.df.iloc[int(idx)]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur √† l'acc√®s iloc[{idx}]\")\n",
    "            raise e\n",
    "\n",
    "        image_path_bdd = self.base_path / row[\"image\"]\n",
    "        label = row[\"label_class\"]\n",
    "\n",
    "        try:\n",
    "            image_bdd = Image.open(image_path_bdd).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur d'ouverture d'image : {image_path_bdd}\")\n",
    "            raise e\n",
    "\n",
    "        if self.transform:\n",
    "            image_bdd = self.transform(image_bdd)\n",
    "        return image_bdd, label"
   ],
   "id": "d8b340fff64f44ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_dataset est un objet PyTorch, une interface permettant de charger dynamiquement les images et labels pour l‚Äôentra√Ænement\n",
    "\n",
    "# Cr√©er les datasets avec la classe WildLensDataset\n",
    "train_dataset = WildLensDataset(train_df, \"../ETL/ressource/image/augmented_train\", transform_train)\n",
    "val_dataset = WildLensDataset(val_df, \"../ETL/ressource/image/augmented_train\", transform_val)\n",
    "test_dataset = WildLensDataset(test_df, \"../ETL/ressource/image/augmented_train\", transform_test)\n",
    "\n",
    "# Cr√©er les DataLoaders avec les bons datasets\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "ab86c19c405dc344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ML.utils.utils import afficher_echantillon\n",
    "\n",
    "afficher_echantillon(train_df, df_all)"
   ],
   "id": "641bfb3e7e2734c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## √âtape 5 : Entra√Ænement du mod√®le\n",
    "\n",
    "Cette section couvre le processus d'entra√Ænement du mod√®le MobileNetV3 sur notre dataset d'animaux sauvages :\n",
    "\n",
    "### Strat√©gie d'entra√Ænement\n",
    "- **Fonction de perte** : Cross-Entropy Loss, adapt√©e aux probl√®mes de classification multi-classes\n",
    "- **Optimiseur** : Adam avec un taux d'apprentissage de 0.001\n",
    "- **Nombre d'√©poques** : 20 passages complets sur l'ensemble d'entra√Ænement\n",
    "- **Sauvegarde du mod√®le** : Conservation du mod√®le avec la meilleure pr√©cision sur l'ensemble de validation\n",
    "\n",
    "### Suivi des performances\n",
    "- Calcul et affichage de la perte (loss) et de la pr√©cision (accuracy) √† chaque √©poque\n",
    "- √âvaluation sur l'ensemble de validation pour d√©tecter le surapprentissage\n",
    "- Visualisation de l'√©volution des m√©triques avec des graphiques\n",
    "\n",
    "### R√©sultats attendus\n",
    "L'entra√Ænement devrait montrer une diminution progressive de la perte et une augmentation de la pr√©cision, avec une convergence vers les meilleures performances apr√®s plusieurs √©poques."
   ],
   "id": "2511ba35-13b9-480e-bec8-75f23f694dc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# D√©tection de l'appareil (GPU ou CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cr√©ation du mod√®le (avec les bons poids pr√©-entra√Æn√©s)\n",
    "mobilenet_v3 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "# Modification de la derni√®re couche pour correspondre au nombre de classes de notre jeu de donn√©es\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, NB_CLASSES)\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "# Fonction de co√ªt et optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v3.parameters(), lr=LR)\n",
    "\n",
    "# Initialisation des listes pour le suivi\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    mobilenet_v3.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # √âvaluation sur validation\n",
    "    mobilenet_v3.eval()\n",
    "    correct_val, total_val, val_running_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mobilenet_v3(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "    # Sauvegarde du meilleur mod√®le\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(mobilenet_v3.state_dict(), \"models\\best_model_wildlens.pt\")\n",
    "        print(f\"üíæ Nouveau meilleur mod√®le sauvegard√© √† l‚Äô√©poque {epoch+1} (val_acc = {val_accuracy:.2f}%)\")\n",
    "\n",
    "    # Journalisation\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NB_EPOCHS} - Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Train Acc: {epoch_accuracy:.2f}% - Val Acc: {val_accuracy:.2f}%\")\n"
   ],
   "id": "12de8bbb2dbb9293",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_losses[:5])\n",
    "print(train_accuracies[:5])\n",
    "print(val_accuracies[:5])\n"
   ],
   "id": "762ae85b608d1cd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "afficher_courbes_entrainement(\n",
    "    train_losses=train_losses,\n",
    "    train_accuracies=train_accuracies,\n",
    "    val_losses=val_losses,\n",
    "    val_accuracies=val_accuracies,\n",
    "    titre=\"Courbes d'entra√Ænement du mod√®le MobileNetV3\"\n",
    ")\n"
   ],
   "id": "f3d24b0ed5e926a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mapping label_class (entier 0 ‚Üí 16) vers nom_fr\n",
    "idx_to_label = {\n",
    "    id_to_class[id_espece]: nom_fr\n",
    "    for id_espece, nom_fr in zip(df_labels[\"id_espece\"], df_labels[\"nom_fr\"])\n",
    "}\n",
    "\n",
    "print(idx_to_label)\n"
   ],
   "id": "498d3dc576b961aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Chargement du mod√®le entra√Æn√©\n",
    "mobilenet_v3.load_state_dict(torch.load(\"models\\best_model_wildlens.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# √âvaluation sur test set\n",
    "correct_test, total_test = 0, 0\n",
    "true_test, pred_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        true_test.extend(labels.cpu().numpy())\n",
    "        pred_test.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f\"‚úÖ Accuracy finale sur le jeu de test : {test_accuracy:.2f}%\")\n"
   ],
   "id": "2c37dea3f9170362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "afficher_matrice_confusion(true_test, pred_test, idx_to_label)",
   "id": "6f064fe91f1fd00f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generer_rapport_classification(true_test, pred_test, idx_to_label)",
   "id": "6ab832227f4b2f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìä Interpr√©tation du tableau de classification par classe\n",
    "\n",
    "Ce tableau r√©sume les performances du mod√®le pour chaque classe du jeu de test. Il comporte les colonnes suivantes :\n",
    "\n",
    "- **Classe** : le nom de l'esp√®ce (ou cat√©gorie) √©valu√©e.\n",
    "\n",
    "- **Pr√©cision (`precision`)** :\n",
    "  - D√©finition : proportion des pr√©dictions correctes parmi toutes les pr√©dictions faites pour cette classe.\n",
    "  - Exemple : Si le mod√®le pr√©dit 10 fois \"chat\", mais que seulement 5 sont correctes, la pr√©cision est 0.50.\n",
    "  - Interpr√©tation : plus la pr√©cision est √©lev√©e, moins le mod√®le fait de **faux positifs** pour cette classe.\n",
    "\n",
    "- **Rappel (`recall`)** :\n",
    "  - D√©finition : proportion des images de cette classe correctement identifi√©es parmi toutes les images r√©ellement de cette classe.\n",
    "  - Exemple : Si 10 \"chat\" sont pr√©sents dans les donn√©es, mais que le mod√®le n'en d√©tecte que 4, le rappel est 0.40.\n",
    "  - Interpr√©tation : plus le rappel est √©lev√©, moins il y a de **faux n√©gatifs**.\n",
    "\n",
    "- **F1-score (`f1-score`)** :\n",
    "  - D√©finition : moyenne harmonique entre la pr√©cision et le rappel.\n",
    "  - Formule : `2 * (precision * recall) / (precision + recall)`\n",
    "  - Interpr√©tation : mesure globale de la performance, utile quand les classes sont d√©s√©quilibr√©es.\n",
    "  - Un bon F1-score indique un bon compromis entre peu de faux positifs et peu de faux n√©gatifs.\n",
    "\n",
    "- **Support (`support`)** :\n",
    "  - D√©finition : nombre d‚Äô√©chantillons r√©els de cette classe dans le jeu de test.\n",
    "  - Utile pour interpr√©ter le poids d‚Äôune classe dans l‚Äô√©valuation globale.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemple : classe `\"Chat\"`\n",
    "\n",
    "- Pr√©cision : 0.45 ‚Üí sur 100 pr√©dictions \"chat\", 45 √©taient correctes\n",
    "- Rappel : 0.43 ‚Üí sur 100 vraies empreintes de chat, 43 ont √©t√© reconnues\n",
    "- F1-score : 0.44 ‚Üí performance globale moyenne\n",
    "- Support : 23 ‚Üí 23 images de chat dans le test\n",
    "\n",
    "Cela montre que le mod√®le a du mal avec cette classe (empreintes trop vari√©es ou peu distinctives).\n",
    "\n"
   ],
   "id": "e82d0c04db85a356"
  },
  {
   "cell_type": "markdown",
   "id": "fbba24f4-6d45-4cce-84ab-3f1b66c9f8da",
   "metadata": {},
   "source": [
    "## √âtape 6 : √âvaluation du mod√®le\n",
    "\n",
    "Cette section √©value les performances du mod√®le entra√Æn√© sur diff√©rents ensembles de donn√©es :\n",
    "\n",
    "### M√©triques d'√©valuation\n",
    "- **Accuracy** : Pourcentage d'images correctement classifi√©es\n",
    "- **Matrice de confusion** : Visualisation d√©taill√©e des pr√©dictions par classe\n",
    "- **Analyse des erreurs** : Identification des classes souvent confondues\n",
    "\n",
    "### Processus d'√©valuation\n",
    "1. √âvaluation sur l'ensemble d'entra√Ænement pour v√©rifier l'apprentissage\n",
    "2. √âvaluation sur l'ensemble de test pour mesurer la g√©n√©ralisation\n",
    "3. Visualisation des r√©sultats avec une matrice de confusion\n",
    "\n",
    "### Interpr√©tation des r√©sultats\n",
    "Les performances sur l'ensemble de test refl√®tent la capacit√© du mod√®le √† g√©n√©raliser √† de nouvelles images. Une diff√©rence importante entre les performances sur l'entra√Ænement et le test peut indiquer un surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "id": "feb21bcd-68a0-4270-9d78-cc590971ddfe",
   "metadata": {},
   "source": [
    "%%timer\n",
    "\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Train Accuracy: {accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4f4c-7330-4330-8fcc-978740ab11b3",
   "metadata": {},
   "source": "* Mesure des performances en TEST en it√©rant sur les donn√©es du dataset `test_loader` :"
  },
  {
   "cell_type": "code",
   "id": "3856e7c6-b8fe-43db-910f-60d5620ac9f0",
   "metadata": {},
   "source": [
    "%%timer\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        predictions = mobilenet_v3(inputs)\n",
    "        predicted = torch.argmax(predictions, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ceea33e3-e62c-4a60-adc0-c2f6b3f0002e",
   "metadata": {},
   "source": [
    "## √âtape 7 : Inf√©rences sur des images individuelles\n",
    "\n",
    "Cette section d√©montre comment utiliser le mod√®le entra√Æn√© pour classifier des images individuelles :\n",
    "\n",
    "### Processus d'inf√©rence\n",
    "1. **Chargement du mod√®le** : Utilisation du meilleur mod√®le sauvegard√©\n",
    "2. **Pr√©traitement de l'image** : Redimensionnement, normalisation et conversion en tenseur\n",
    "3. **Pr√©diction** : Passage de l'image dans le mod√®le pour obtenir les probabilit√©s de classe\n",
    "4. **Interpr√©tation** : Affichage de la classe pr√©dite et du score de confiance\n",
    "\n",
    "### Application pratique\n",
    "Cette fonctionnalit√© est essentielle pour l'application mobile WildLens, o√π les utilisateurs pourront prendre des photos d'animaux et obtenir une identification en temps r√©el. Le code pr√©sent√© ici sert de base pour l'impl√©mentation de cette fonctionnalit√© dans l'application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9721f7-c1fc-4a2e-9e12-1a475d22e3ca",
   "metadata": {},
   "source": [
    "* Nouvelle √©valuation globale sur les donn√©es de TEST en it√©rant manuellement sur les images physiques :"
   ]
  },
  {
   "cell_type": "code",
   "id": "74812557-01a5-43c3-9bb2-cb45ec0b5df3",
   "metadata": {},
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚úÖ Recharge le mod√®le\n",
    "mobilenet_v3.load_state_dict(torch.load(\"models\\best_model_wildlens.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# üìÅ Base du dossier image\n",
    "base_path = Path(\"../ETL/ressource/image/augmented_train\")\n",
    "\n",
    "# üé≤ S√©lection al√©atoire d‚Äôune classe et d‚Äôune image\n",
    "all_classes = [d for d in base_path.iterdir() if d.is_dir()]\n",
    "chosen_class = random.choice(all_classes)\n",
    "chosen_image = random.choice(list(chosen_class.glob(\"*.jpg\")))\n",
    "\n",
    "print(f\"üì∏ Image s√©lectionn√©e : {chosen_image}\")\n",
    "\n",
    "# üîÑ Pr√©traitement\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image = Image.open(chosen_image).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# üîÆ Pr√©diction\n",
    "with torch.no_grad():\n",
    "    output = mobilenet_v3(input_tensor)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    confidence, predicted_class = torch.max(probs, dim=1)\n",
    "\n",
    "# üñºÔ∏è Affichage image + pr√©diction\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Pr√©diction : {idx_to_label[predicted_class.item()]}\\nConfiance : {confidence.item()*100:.2f}%\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine tuning",
   "id": "54913e2cc44ef57c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Rechargement du mod√®le pr√©-entra√Æn√©\n",
    "mobilenet_v3 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, 17)\n",
    "mobilenet_v3.load_state_dict(torch.load(\"models\\best_model_wildlens.pt\"))\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "# D√©bloquer toutes les couches pour fine-tuning\n",
    "for param in mobilenet_v3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimiseur et loss\n",
    "optimizer = optim.Adam(mobilenet_v3.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Logs pour les courbes\n",
    "fine_train_losses = []\n",
    "fine_train_accuracies = []\n",
    "fine_val_losses = []\n",
    "fine_val_accuracies = []\n",
    "\n",
    "# Boucle fine-tuning\n",
    "for epoch in range(NB_FINE_EPOCHS):\n",
    "    mobilenet_v3.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    mobilenet_v3.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mobilenet_v3(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    # Log\n",
    "    fine_train_losses.append(train_loss)\n",
    "    fine_train_accuracies.append(train_acc)\n",
    "    fine_val_losses.append(val_loss)\n",
    "    fine_val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NB_FINE_EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Sauvegarde du mod√®le affin√©\n",
    "torch.save(mobilenet_v3.state_dict(), \"models\\best_model_finetuned_wildlens.pt\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model saved as 'best_model_finetuned_wildlens.pt'\")\n",
    "\n",
    "\n",
    "afficher_courbes_entrainement(\n",
    "    train_losses=fine_train_losses,\n",
    "    train_accuracies=fine_train_accuracies,\n",
    "    val_losses=fine_val_losses,\n",
    "    val_accuracies=fine_val_accuracies,\n",
    "    titre=\"Courbes fine-tuning du mod√®le MobileNetV3\"\n",
    ")\n"
   ],
   "id": "b8e415d0b5e0e927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "afficher_courbes_entrainement(\n",
    "    train_losses=train_losses,\n",
    "    train_accuracies=train_accuracies,\n",
    "    val_losses=val_losses,\n",
    "    val_accuracies=val_accuracies,\n",
    "    titre=\"Courbes d'entra√Ænement du mod√®le MobileNetV3\"\n",
    ")\n",
    "\n",
    "afficher_courbes_entrainement(\n",
    "    train_losses=fine_train_losses,\n",
    "    train_accuracies=fine_train_accuracies,\n",
    "    val_losses=fine_val_losses,\n",
    "    val_accuracies=fine_val_accuracies,\n",
    "    titre=\"Courbes fine-tuning du mod√®le MobileNetV3\"\n",
    ")\n",
    "\n",
    "# √âvaluation du mod√®le fine-tun√© sur le test set\n",
    "mobilenet_v3.eval()\n",
    "true_labels_ft, predicted_labels_ft = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels_ft.extend(labels.cpu().numpy())\n",
    "        predicted_labels_ft.extend(preds.cpu().numpy())\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "afficher_matrice_confusion(\n",
    "    true_labels=true_labels_ft,\n",
    "    predicted_labels=predicted_labels_ft,\n",
    "    idx_to_label=idx_to_label,\n",
    "    titre=\"Matrice de confusion - Mod√®le fine-tun√©\"\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels_ft, predicted_labels_ft,\n",
    "                            target_names=[idx_to_label[i] for i in range(len(idx_to_label))]))\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# S'assurer que le mod√®le est bien en mode √©valuation\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# √âvaluation sur le jeu d'entra√Ænement\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "train_accuracy = 100 * correct / total\n",
    "print(f\"‚úÖ Train Accuracy apr√®s fine-tuning : {train_accuracy:.2f}%\")\n",
    "\n",
    "# √âvaluation sur le jeu de test\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        predictions = mobilenet_v3(inputs)\n",
    "        predicted = torch.argmax(predictions, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"‚úÖ Test Accuracy finale : {test_accuracy:.2f}%\")"
   ],
   "id": "949e9884a7a9dbfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models\n",
    "#\n",
    "# # üì¶ Charger le mod√®le entra√Æn√© (t√™te seule)\n",
    "# mobilenet_v3 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "# mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, 17)\n",
    "# mobilenet_v3.load_state_dict(torch.load(\"best_model_wildlens.pt\"))\n",
    "# mobilenet_v3.eval()\n",
    "#\n",
    "# # üß™ Entr√©e factice pour tracer le mod√®le\n",
    "# dummy_input = torch.randn(1, 3, 224, 224)\n",
    "#\n",
    "# # üßµ Tracer le mod√®le avec TorchScript\n",
    "# scripted_model = torch.jit.trace(mobilenet_v3, dummy_input)\n",
    "#\n",
    "# # üíæ Sauvegarde avec nom explicite pour Android\n",
    "# scripted_model.save(\"mobilenetv3_wildlens_android.pt\")\n",
    "# print(\"‚úÖ Mod√®le export√© pour Android : mobilenetv3_wildlens_android.pt\")\n"
   ],
   "id": "8f6f16b1f28a734a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fin_notebook = time.time()\n",
    "duree_totale = fin_notebook - debut_notebook\n",
    "\n",
    "minutes, secondes = divmod(duree_totale, 60)\n",
    "print(f\"‚è±Ô∏è Temps total d'ex√©cution du notebook : {int(minutes)} min {int(secondes)} sec\")\n"
   ],
   "id": "7ae93d545b44fa70",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
