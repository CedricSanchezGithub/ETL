{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fee02c-ad40-40d1-973b-ac3ad9cc5d90",
   "metadata": {},
   "source": [
    "1# Classification multiclasses des images du dataset 5 flowers\n",
    "\n",
    "# Data Augmentation + Transfer Learning avec MobileNetV3Small + Fine Tuning\n",
    "\n",
    "- Auteur : Laurent PISSOT\n",
    "- Date : 12 Mai 2025\n",
    "- Statut : **Valid√©**\n",
    "- R√©f√©rences : <br>https://docs.pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_small.html, <br> https://pytorch.org/blog/torchvision-mobilenet-v3-implementation/..., <br> https://docs.pytorch.org/vision/0.22/models.html, <br> https://pypi.org/project/torch/2.6.0/, <br> requ√™tes Copilot !"
   ]
  },
  {
   "cell_type": "code",
   "id": "863496cc-659d-4ed5-8472-6a9c98008afc",
   "metadata": {},
   "source": [
    "!python --version"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60f9fbf7-03d1-4b71-a31a-a63de611b477",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4402f02-de6f-4641-b5c6-1abb67ffe94a",
   "metadata": {},
   "source": [
    "MobileNetV3, une architecture de pointe pour des mod√®les de deep learning efficaces con√ßus pour les appareils mobiles. Il s‚Äôagit de la troisi√®me g√©n√©ration de la famille MobileNet.\n",
    "\n",
    "Les MobileNet sont des r√©seaux neuronaux convolutifs (CNN) l√©gers optimis√©s pour la vitesse et la pr√©cision. MobileNetV3 introduit de nouvelles am√©liorations de l‚Äôarchitecture, telles que la recherche d‚Äôarchitecture neuronale (NAS) sensible √† la plate-forme et NetAdapt, afin d‚Äôam√©liorer encore les performances.\n",
    "\n",
    "**Qu'est-ce que MobileNet ?**<br>\n",
    "MobileNet est une famille de r√©seaux neuronaux con√ßus pour une inf√©rence efficace sur les appareils mobiles et int√©gr√©s. Le MobileNetV1 original a introduit une technique appel√©e convolutions s√©parables en profondeur, qui a consid√©rablement r√©duit le nombre de calculs par rapport aux convolutions traditionnelles.\n",
    "\n",
    "Les MobileNet sont particuli√®rement bien adapt√©s aux t√¢ches telles que la classification d‚Äôimages, la d√©tection d‚Äôobjets et la segmentation s√©mantique sur des appareils disposant d‚Äôune puissance de calcul limit√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8b102-e7d9-41a8-9a18-5c5521bbe579",
   "metadata": {},
   "source": [
    "**MobileNetV1 vs V2 vs V3 : quelle est la diff√©rence ?**\n",
    "\n",
    "**MobileNetV1** : Introduction de convolutions s√©parables en profondeur pour r√©duire le calcul et la taille du mod√®le.\n",
    "\n",
    "**MobileNetV2** : Ajout de r√©sidus invers√©s et de goulets d‚Äô√©tranglement lin√©aires pour rendre le r√©seau plus efficace.\n",
    "\n",
    "**MobileNetV3** : Combine le meilleur des deux versions pr√©c√©dentes et les am√©liore avec :\n",
    "\n",
    "- NAS sensible √† la plate-forme pour optimiser l‚Äôarchitecture des processeurs mobiles.\n",
    "- NetAdapt pour affiner les couches r√©seau pour plus d‚Äôefficacit√©.\n",
    "- Modules Squeeze-and-Excite (SE) pour stimuler l‚Äôapprentissage des fonctionnalit√©s.\n",
    "- Fonction d‚Äôactivation H-Swish pour am√©liorer l‚Äôefficacit√© du mod√®le."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timer(line, cell):\n",
    "    start = time.time()\n",
    "    exec(cell, globals())\n",
    "    end = time.time()\n",
    "    print(f\"‚è± Temps d'ex√©cution de la cellule : {end - start:.2f} secondes\")\n",
    "\n"
   ],
   "id": "363bdc0c9985d0ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14cacd11-e26f-49ac-b686-c7942f5b3c36",
   "metadata": {},
   "source": [
    "## √âtape 1 : Configuration de l'environnement d'ex√©cution"
   ]
  },
  {
   "cell_type": "code",
   "id": "234ca7c5-7f20-458c-8db5-999780c8178f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pathlib\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "start_time_notebook = time.time()\n",
    "# D√©finition des constantes\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 10\n",
    "NB_CLASSES = 17"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c23b0705-e922-4c77-b245-d2798c92d0ff",
   "metadata": {},
   "source": [
    "torch.__version__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ce561af-4085-4890-aa4b-40d94a0e67be",
   "metadata": {},
   "source": [
    "#### D√©finition des fonctions locales"
   ]
  },
  {
   "cell_type": "code",
   "id": "09a57d7d-933f-498d-b6f9-01327430e9cf",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
    "def f_make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
    "    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "      If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "      will be used.\n",
    "\n",
    "      Args:\n",
    "        y_true: Array of truth labels (must be same shape as y_pred).\n",
    "        y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "        classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "        figsize: Size of output figure (default=(10, 10)).\n",
    "        text_size: Size of output figure text (default=15).\n",
    "        norm: normalize values or not (default=False).\n",
    "        savefig: save confusion matrix to file (default=False).\n",
    "  \n",
    "      Returns:\n",
    "        A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "\n",
    "      Example usage:\n",
    "        make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
    "                              y_pred=y_preds, # predicted labels\n",
    "                              classes=class_names, # array of class label names\n",
    "                              figsize=(15, 15),\n",
    "                              text_size=10)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create the confustion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "    n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "    # Plot the figure and make it pretty\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "  \n",
    "    # Are there a list of classes?\n",
    "    if classes:  labels = classes\n",
    "    else:  labels = np.arange(cm.shape[0])\n",
    "  \n",
    "    # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           ylabel=\"True label\",\n",
    "           xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "           yticks=np.arange(n_classes), \n",
    "           xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "           yticklabels=labels)\n",
    "  \n",
    "    # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
    "    plt.xticks(rotation=70, fontsize=text_size)\n",
    "    plt.yticks(fontsize=text_size)\n",
    "\n",
    "    # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if norm:\n",
    "            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                     size=text_size)\n",
    "        else:\n",
    "           plt.text(j, i, f\"{cm[i, j]}\",\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                    size=text_size)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2c5a4c4-4cda-4bda-b04d-928be8ef1ee5",
   "metadata": {},
   "source": [
    "%%timer\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Transformations standards pour MobileNetV3\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # redimensionnement obligatoire\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Pas d‚Äôaugmentation, donc m√™mes transformations pour val/test\n",
    "transform_val = transform_train\n",
    "transform_test = transform_train"
   ],
   "id": "6bf7b858ef8cbedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chargement des donn√©es",
   "id": "fbf830bd00278afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "engine = create_engine(\"mysql+pymysql://root:root@localhost:3306/wildlens\")\n",
    "df_all = pd.read_sql(\"SELECT * FROM wildlens_images\", engine)\n",
    "df_labels = pd.read_sql(\"SELECT id_espece, nom_fr FROM wildlens_facts\", engine)\n",
    "df_all = pd.merge(df_all, df_labels, on=\"id_espece\", how=\"left\")\n",
    "\n",
    "# √âtape 1 : r√©cup√©rer les ID d'esp√®ce uniques (par s√©curit√©)\n",
    "unique_species_ids = sorted(df_all['id_espece'].unique())\n",
    "\n",
    "# √âtape 2 : dictionnaire de mappage\n",
    "id_to_class = {id_: idx for idx, id_ in enumerate(unique_species_ids)}\n",
    "\n",
    "# √âtape 3 : conversion dans le DataFrame\n",
    "df_all['label_class'] = df_all['id_espece'].map(id_to_class)\n",
    "\n",
    "# Split + copie propre\n",
    "train_df = df_all[df_all[\"id_etat\"] == 1].copy()\n",
    "val_df = df_all[df_all[\"id_etat\"] == 2].copy()\n",
    "test_df = df_all[df_all[\"id_etat\"] == 3].copy()\n",
    "\n"
   ],
   "id": "88bcfd6558e53751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WildLensDataset(Dataset):\n",
    "    def __init__(self, dataframe, base_path, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.base_path = Path(base_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Forcer index entier (r√©sout 90% des cas)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "        elif isinstance(idx, (list, tuple)):\n",
    "            idx = idx[0]\n",
    "\n",
    "        # ‚ö†Ô∏è Debug temporaire : afficher l'index et taille max\n",
    "        if idx >= len(self.df):\n",
    "            raise IndexError(f\"Index {idx} hors limites (longueur dataset : {len(self.df)})\")\n",
    "\n",
    "        try:\n",
    "            row = self.df.iloc[int(idx)]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur √† l'acc√®s iloc[{idx}]\")\n",
    "            raise e\n",
    "\n",
    "        image_path_bdd = self.base_path / row[\"image\"]\n",
    "        label = row[\"label_class\"]\n",
    "\n",
    "        try:\n",
    "            image_bdd = Image.open(image_path_bdd).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur d'ouverture d'image : {image_path_bdd}\")\n",
    "            raise e\n",
    "\n",
    "        if self.transform:\n",
    "            image_bdd = self.transform(image_bdd)\n",
    "        return image_bdd, label\n",
    "\n",
    "\n",
    "# train_dataset est un objet PyTorch, une interface permettant de charger dynamiquement les images et labels pour l‚Äôentra√Ænement\n",
    "\n",
    "# Cr√©er les datasets avec votre classe WildLensDataset\n",
    "train_dataset = WildLensDataset(train_df, \"../ETL/ressource/image/augmented_train\", transform_train)\n",
    "val_dataset = WildLensDataset(val_df, \"../ETL/ressource/image/augmented_train\", transform_val)\n",
    "test_dataset = WildLensDataset(test_df, \"../ETL/ressource/image/augmented_train\", transform_test)\n",
    "\n",
    "# Cr√©er les DataLoaders avec les bons datasets\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ],
   "id": "d8b340fff64f44ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üîç Test d‚Äôacc√®s direct sans DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "img, lbl = train_dataset[0]\n",
    "print(\"Image OK :\", img.shape)\n",
    "print(\"Label OK :\", lbl)\n",
    "\n",
    "img, lbl = val_dataset[0]\n",
    "print(\"‚úîÔ∏è Test val_dataset[0] ok :\", img.shape, lbl)\n",
    "\n"
   ],
   "id": "e851e992e581d296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_df.head())\n",
    "print(train_df.columns)\n"
   ],
   "id": "525834ba6d56a881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# R√©partition des classes dans le train\n",
    "class_counts = train_df[\"nom_fr\"].value_counts().sort_index()\n",
    "class_counts.plot(kind=\"bar\", figsize=(10, 4))\n",
    "plt.title(\"R√©partition des images par classe (nom_fr) - TRAIN\")\n",
    "plt.xlabel(\"Animaux\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "df_all[\"id_etat\"].replace({1: \"train\", 2: \"val\", 3: \"test\"}).value_counts().plot(\n",
    "    kind=\"bar\", title=\"R√©partition globale des images par split\"\n",
    ")\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df_all[\"split\"] = df_all[\"id_etat\"].replace({1: \"train\", 2: \"val\", 3: \"test\"})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df_all, x=\"nom_fr\", hue=\"split\")\n",
    "plt.title(\"R√©partition des images par classe et split\")\n",
    "plt.xlabel(\"ID de l'esp√®ce\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.legend(title=\"Split\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f8f07a12920e9703",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f286c53-1fe6-44fd-ab33-dd42676d3638",
   "metadata": {},
   "source": [
    "## √âtape 2 : Chargement du mod√®le pr√© entrain√© MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "id": "37015d8c-3eec-444a-924c-3d93a4cff5a1",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Load MobileNetV3-Large pretrained on ImageNet\n",
    "# Pre-trained Model: The pretrained=True argument loads a model trained on ImageNet.\n",
    "#mobilenet_v3_large = models.mobilenet_v3_large(pretrained=True)  # Use mobilenet_v3_small for the smaller version\n",
    "mobilenet_v3 = models.mobilenet_v3_small(pretrained=True, weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "# Modify the final layer for a custom number of classes (e.g., 10)\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, NB_CLASSES)   # mobilenet_v3_small\n",
    "\n",
    "# Verify the modified model\n",
    "print(mobilenet_v3)\n",
    "\n",
    "#train on GPU if CUDA is available, else on CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "#print training information\n",
    "print(\"\")\n",
    "if torch.cuda.is_available():  hardware = \"GPU \" + str(device) \n",
    "else:    hardware = \"CPU (CUDA was not found)\" \n",
    "print(\"Training information:\")\n",
    "print(\"iterations:\", NB_EPOCHS)\n",
    "print(\"batch size:\", BATCH_SIZE)\n",
    "print(\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2511ba35-13b9-480e-bec8-75f23f694dc6",
   "metadata": {},
   "source": [
    "### √âtape 3 : R√©glage de pr√©cision du mod√®le MobileNetV3 sur les couches de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406f58c-f359-4d8b-be7b-fe8114ae40fa",
   "metadata": {},
   "source": [
    "* Pr√©traitement et enrichissement des donn√©es (Data Augmentation) :"
   ]
  },
  {
   "cell_type": "code",
   "id": "87d7da81-4d81-42a3-919b-a597c941ef59",
   "metadata": {},
   "source": [
    "# DIR_IMG_TRAIN = '../ETL/ressource/image/augmented_train'  # localisation a adapter\n",
    "# DIR_IMG_TEST = 'flower_photos_test'  # localisation a adapter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "debffc3b-0b75-4316-a55b-25e5ff731cd5",
   "metadata": {},
   "source": [
    "# %%timer\n",
    "# # Pour l'entra√Ænement (avec augmentation)\n",
    "# # 1. Image Preprocessing: The input image must be resized, normalized, and converted to a tensor.\n",
    "# NORM_MEANS = (0.485, 0.456, 0.406) #precomputed channel means of ImageNet(train) for normalization\n",
    "# NORM_STDS = (0.229, 0.224, 0.225) #precomputed standard deviations\n",
    "#\n",
    "# train_transform = transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),  # Resize image to 224x224\n",
    "#                                       #transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.RandomRotation(degrees=15),\n",
    "#                                       transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize(mean=NORM_MEANS, std=NORM_STDS)])\n",
    "#\n",
    "# # Load dataset et configure DataLoaders\n",
    "# image_train_path = DIR_IMG_TRAIN\n",
    "# train_dataset = datasets.ImageFolder(root=image_train_path, transform=train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f48e4a6-1133-4bfb-b325-123c86e0993d",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Constantes pour l'apprentissage\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 10\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32  # ou la valeur que tu veux\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc9ad512-b928-4ae3-b653-4bfb50c05dd4",
   "metadata": {},
   "source": [
    "%%timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Constantes\n",
    "LR = 0.001\n",
    "NB_EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Mod√®le\n",
    "mobilenet_v3 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "mobilenet_v3.classifier[3] = nn.Linear(mobilenet_v3.classifier[3].in_features, 17)\n",
    "mobilenet_v3 = mobilenet_v3.to(device)\n",
    "\n",
    "# Optimiseur & fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v3.parameters(), lr=LR)\n",
    "\n",
    "# Suivi\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0  # suivi du meilleur score de validation\n",
    "\n",
    "# Entra√Ænement\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    mobilenet_v3.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # √âvaluation validation\n",
    "    mobilenet_v3.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mobilenet_v3(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Sauvegarde du meilleur mod√®le\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(mobilenet_v3.state_dict(), \"best_model_wildlens.pt\")\n",
    "        print(f\"üíæ Nouveau meilleur mod√®le sauvegard√© √† l‚Äô√©poque {epoch+1} (val_acc = {val_accuracy:.2f}%)\")\n",
    "\n",
    "    # Log\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NB_EPOCHS} - Loss: {epoch_loss:.4f} - Train Acc: {epoch_accuracy:.2f}% - Val Acc: {val_accuracy:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üìä Courbes\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.title(\"Train Loss par √©poque\")\n",
    "plt.xlabel(\"√âpoque\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Acc\", marker='o')\n",
    "plt.plot(val_accuracies, label=\"Val Acc\", marker='s')\n",
    "plt.title(\"Accuracy (Train vs Val)\")\n",
    "plt.xlabel(\"√âpoque\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b7ca6bb9f5bfbc81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mapping label_class (entier 0 ‚Üí 16) vers nom_fr\n",
    "idx_to_label = {\n",
    "    id_to_class[id_espece]: nom_fr\n",
    "    for id_espece, nom_fr in zip(df_labels[\"id_espece\"], df_labels[\"nom_fr\"])\n",
    "}\n"
   ],
   "id": "498d3dc576b961aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Chargement du mod√®le entra√Æn√©\n",
    "mobilenet_v3.load_state_dict(torch.load(\"best_model_wildlens.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# √âvaluation sur test set\n",
    "correct_test, total_test = 0, 0\n",
    "true_test, pred_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        true_test.extend(labels.cpu().numpy())\n",
    "        pred_test.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f\"‚úÖ Accuracy finale sur le jeu de test : {test_accuracy:.2f}%\")\n"
   ],
   "id": "2c37dea3f9170362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(true_test, pred_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[idx_to_label[i] for i in range(len(cm))])\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(xticks_rotation='vertical', ax=ax, cmap=\"Greens\")\n",
    "plt.title(\"üß™ Matrice de confusion - Test final\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "6f064fe91f1fd00f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbba24f4-6d45-4cce-84ab-3f1b66c9f8da",
   "metadata": {},
   "source": [
    "### √âtape 4 : √âvaluation du mod√®le :"
   ]
  },
  {
   "cell_type": "code",
   "id": "feb21bcd-68a0-4270-9d78-cc590971ddfe",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Evaluation Mode: Always set the model to evaluation mode using model.eval() to disable dropout and batch normalization updates.\n",
    "mobilenet_v3.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = mobilenet_v3(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Train Accuracy: {accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4f4c-7330-4330-8fcc-978740ab11b3",
   "metadata": {},
   "source": [
    "* Mesure des performances en TEST en it√©rant sur les donn√©es du dataset `test_loader` :"
   ]
  },
  {
   "cell_type": "code",
   "id": "3856e7c6-b8fe-43db-910f-60d5620ac9f0",
   "metadata": {},
   "source": [
    "%%timer\n",
    "# Predict (Make Predictions on New Data)\n",
    "#mobilenet_v3.eval()  # Set to evaluation mode\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():  # Disable gradient tracking for efficiency\n",
    "    for inputs, labels in test_loader:  # Assuming `test_loader` is your DataLoader\n",
    "        predictions = mobilenet_v3(inputs)  # Forward pass\n",
    "        predicted = torch.argmax(predictions, dim=1)  # Get class labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ceea33e3-e62c-4a60-adc0-c2f6b3f0002e",
   "metadata": {},
   "source": [
    "### Etape 5 : Inf√©rences unitaires et globales √† l'ensemble de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9721f7-c1fc-4a2e-9e12-1a475d22e3ca",
   "metadata": {},
   "source": [
    "* Nouvelle √©valuation globale sur les donn√©es de TEST en it√©rant manuellement sur les images physiques :"
   ]
  },
  {
   "cell_type": "code",
   "id": "74812557-01a5-43c3-9bb2-cb45ec0b5df3",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ‚úÖ Charge le mod√®le entra√Æn√©\n",
    "mobilenet_v3.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "mobilenet_v3.eval()\n",
    "\n",
    "# üñºÔ∏è Chemin de l‚Äôimage √† tester\n",
    "image_path = \"../ETL/ressource/image/augmented_train/muledeer/aug_0_2097.jpg\"  # ‚Üê √Ä modifier\n",
    "\n",
    "# üîÑ Pr√©traitement\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# üîÆ Pr√©diction\n",
    "with torch.no_grad():\n",
    "    output = mobilenet_v3(input_tensor)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    confidence, predicted_class = torch.max(probs, dim=1)\n",
    "\n",
    "# üßæ Affichage\n",
    "print(f\"‚úÖ Classe pr√©dite : {idx_to_label[predicted_class.item()]}\")\n",
    "print(f\"üî¢ Score de confiance : {confidence.item()*100:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25ae6ce6-f93b-4c42-941e-1eee7de2e2a5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
